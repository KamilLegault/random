{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Downloads/train/\"\n",
    "num_workers = 2\n",
    "batch_size = 64\n",
    "img_size = 64\n",
    "latent_dim = 100\n",
    "n_gen = 32\n",
    "n_disc = 32\n",
    "n_epochs = 25\n",
    "lr = 0.0002\n",
    "beta = 0.5\n",
    "use_cuda = True\n",
    "num_gpu = 1\n",
    "path_gen = \"\"\n",
    "path_disc = \"\"\n",
    "path_output = \"output\"\n",
    "nc = 3\n",
    "try:\n",
    "    os.makedirs(path_output)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root=data_path,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(img_size),\n",
    "                                   transforms.CenterCrop(img_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=int(num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     latent_dim, n_gen * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(n_gen * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(n_gen * 8, n_gen * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_gen * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(n_gen * 4, n_gen * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_gen * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(n_gen * 2,     n_gen, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_gen),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    n_gen,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "            output = self.main(input)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netG(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = _netG()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, n_disc, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(n_disc, n_disc * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_disc * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(n_disc * 2, n_disc * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_disc * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(n_disc * 4, n_disc * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_disc * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(n_disc * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = _netD()\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "input = torch.FloatTensor(batch_size, 3, img_size, img_size)\n",
    "noise = torch.FloatTensor(batch_size, latent_dim, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, latent_dim, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "if use_cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000][0/3166] Loss_D: 0.2802 Loss_G: 2.4473 D(x): 0.8743 D(G(z)): 0.0334 / 0.2331\n",
      "[0/1000][100/3166] Loss_D: 3.0553 Loss_G: 3.6992 D(x): 0.8370 D(G(z)): 0.7798 / 0.1231\n",
      "[0/1000][200/3166] Loss_D: 12.9355 Loss_G: 0.0420 D(x): 0.0001 D(G(z)): 0.0061 / 0.9719\n",
      "[0/1000][300/3166] Loss_D: 2.8710 Loss_G: 6.4311 D(x): 0.9218 D(G(z)): 0.8113 / 0.0094\n",
      "[0/1000][400/3166] Loss_D: 5.2961 Loss_G: 0.3384 D(x): 0.3002 D(G(z)): 0.6742 / 0.7921\n",
      "[0/1000][500/3166] Loss_D: 9.5823 Loss_G: 2.2678 D(x): 0.0127 D(G(z)): 0.0136 / 0.3839\n",
      "[0/1000][600/3166] Loss_D: 5.5631 Loss_G: 0.9314 D(x): 0.4253 D(G(z)): 0.8567 / 0.6084\n",
      "[0/1000][700/3166] Loss_D: 6.1151 Loss_G: 0.9752 D(x): 0.1863 D(G(z)): 0.8024 / 0.5177\n",
      "[0/1000][800/3166] Loss_D: 9.9938 Loss_G: 0.0008 D(x): 0.0127 D(G(z)): 0.4026 / 0.9992\n",
      "[0/1000][900/3166] Loss_D: 6.1183 Loss_G: 0.9759 D(x): 0.3670 D(G(z)): 0.9460 / 0.4828\n",
      "[0/1000][1000/3166] Loss_D: 13.3637 Loss_G: 0.0367 D(x): 0.8628 D(G(z)): 1.0000 / 0.9682\n",
      "[0/1000][1100/3166] Loss_D: 11.1963 Loss_G: 0.0307 D(x): 0.0171 D(G(z)): 0.7745 / 0.9718\n",
      "[0/1000][1200/3166] Loss_D: 7.9917 Loss_G: 0.0675 D(x): 0.0414 D(G(z)): 0.8097 / 0.9404\n",
      "[0/1000][1300/3166] Loss_D: 8.1871 Loss_G: 0.3248 D(x): 0.0687 D(G(z)): 0.8873 / 0.7709\n",
      "[0/1000][1400/3166] Loss_D: 8.4540 Loss_G: 0.0825 D(x): 0.0999 D(G(z)): 0.9507 / 0.9295\n",
      "[0/1000][1500/3166] Loss_D: 6.9091 Loss_G: 0.1542 D(x): 0.0896 D(G(z)): 0.8651 / 0.8738\n",
      "[0/1000][1600/3166] Loss_D: 6.6032 Loss_G: 0.2156 D(x): 0.1453 D(G(z)): 0.8434 / 0.8425\n",
      "[0/1000][1700/3166] Loss_D: 7.7220 Loss_G: 0.0874 D(x): 0.1269 D(G(z)): 0.9294 / 0.9210\n",
      "[0/1000][1800/3166] Loss_D: 6.6543 Loss_G: 0.2620 D(x): 0.1286 D(G(z)): 0.8428 / 0.8238\n",
      "[0/1000][1900/3166] Loss_D: 6.1595 Loss_G: 0.2009 D(x): 0.1286 D(G(z)): 0.7608 / 0.8373\n",
      "[0/1000][2000/3166] Loss_D: 6.0328 Loss_G: 0.1239 D(x): 0.1378 D(G(z)): 0.8182 / 0.8914\n",
      "[0/1000][2100/3166] Loss_D: 8.2633 Loss_G: 0.1395 D(x): 0.0742 D(G(z)): 0.8988 / 0.8856\n",
      "[0/1000][2200/3166] Loss_D: 5.6826 Loss_G: 0.2687 D(x): 0.2423 D(G(z)): 0.8255 / 0.8126\n",
      "[0/1000][2300/3166] Loss_D: 9.7028 Loss_G: 0.1250 D(x): 0.0512 D(G(z)): 0.9116 / 0.9008\n",
      "[0/1000][2400/3166] Loss_D: 8.0756 Loss_G: 0.1939 D(x): 0.0652 D(G(z)): 0.8624 / 0.8404\n",
      "[0/1000][2500/3166] Loss_D: 7.4620 Loss_G: 0.1578 D(x): 0.1330 D(G(z)): 0.9394 / 0.8734\n",
      "[0/1000][2600/3166] Loss_D: 7.8521 Loss_G: 0.2010 D(x): 0.1454 D(G(z)): 0.9269 / 0.8702\n",
      "[0/1000][2700/3166] Loss_D: 5.5930 Loss_G: 0.2523 D(x): 0.2275 D(G(z)): 0.8333 / 0.8243\n",
      "[0/1000][2800/3166] Loss_D: 6.4451 Loss_G: 0.4337 D(x): 0.1273 D(G(z)): 0.8054 / 0.7038\n",
      "[0/1000][2900/3166] Loss_D: 6.7364 Loss_G: 0.1480 D(x): 0.1390 D(G(z)): 0.8585 / 0.8808\n",
      "[0/1000][3000/3166] Loss_D: 6.7497 Loss_G: 0.1961 D(x): 0.1594 D(G(z)): 0.9150 / 0.8971\n",
      "[0/1000][3100/3166] Loss_D: 6.4792 Loss_G: 0.1774 D(x): 0.1807 D(G(z)): 0.8712 / 0.8636\n",
      "[1/1000][0/3166] Loss_D: 7.0885 Loss_G: 0.2257 D(x): 0.1585 D(G(z)): 0.9243 / 0.8344\n",
      "[1/1000][100/3166] Loss_D: 8.2921 Loss_G: 0.2624 D(x): 0.0862 D(G(z)): 0.8899 / 0.8092\n",
      "[1/1000][200/3166] Loss_D: 4.6659 Loss_G: 0.4524 D(x): 0.3593 D(G(z)): 0.8866 / 0.7088\n",
      "[1/1000][300/3166] Loss_D: 6.0948 Loss_G: 2.2995 D(x): 0.4223 D(G(z)): 0.9596 / 0.2085\n",
      "[1/1000][400/3166] Loss_D: 8.4860 Loss_G: 0.3931 D(x): 0.0426 D(G(z)): 0.8064 / 0.7693\n",
      "[1/1000][500/3166] Loss_D: 6.2197 Loss_G: 0.1434 D(x): 0.1142 D(G(z)): 0.7360 / 0.8816\n",
      "[1/1000][600/3166] Loss_D: 8.8282 Loss_G: 0.1248 D(x): 0.1285 D(G(z)): 0.9383 / 0.8961\n",
      "[1/1000][700/3166] Loss_D: 6.1989 Loss_G: 0.1751 D(x): 0.1840 D(G(z)): 0.9038 / 0.8558\n",
      "[1/1000][800/3166] Loss_D: 8.6770 Loss_G: 0.0934 D(x): 0.1099 D(G(z)): 0.9311 / 0.9223\n",
      "[1/1000][900/3166] Loss_D: 6.6363 Loss_G: 0.3244 D(x): 0.1645 D(G(z)): 0.8626 / 0.8274\n",
      "[1/1000][1000/3166] Loss_D: 7.3704 Loss_G: 0.1048 D(x): 0.0998 D(G(z)): 0.9293 / 0.9083\n",
      "[1/1000][1100/3166] Loss_D: 6.2661 Loss_G: 0.1832 D(x): 0.1764 D(G(z)): 0.9114 / 0.8450\n",
      "[1/1000][1200/3166] Loss_D: 5.7170 Loss_G: 0.1925 D(x): 0.2747 D(G(z)): 0.8974 / 0.8407\n",
      "[1/1000][1300/3166] Loss_D: 6.6787 Loss_G: 0.1111 D(x): 0.0682 D(G(z)): 0.7492 / 0.9075\n",
      "[1/1000][1400/3166] Loss_D: 5.7318 Loss_G: 0.4555 D(x): 0.1009 D(G(z)): 0.7165 / 0.7011\n",
      "[1/1000][1500/3166] Loss_D: 7.0331 Loss_G: 0.4542 D(x): 0.0363 D(G(z)): 0.7870 / 0.7062\n",
      "[1/1000][1600/3166] Loss_D: 5.5357 Loss_G: 0.5483 D(x): 0.1974 D(G(z)): 0.8506 / 0.6609\n",
      "[1/1000][1700/3166] Loss_D: 8.9081 Loss_G: 0.2727 D(x): 0.0811 D(G(z)): 0.9492 / 0.8126\n",
      "[1/1000][1800/3166] Loss_D: 6.7986 Loss_G: 0.3270 D(x): 0.1037 D(G(z)): 0.8220 / 0.7598\n",
      "[1/1000][1900/3166] Loss_D: 4.3868 Loss_G: 2.9767 D(x): 0.3151 D(G(z)): 0.8190 / 0.1161\n",
      "[1/1000][2000/3166] Loss_D: 6.4446 Loss_G: 0.4114 D(x): 0.0915 D(G(z)): 0.8514 / 0.6918\n",
      "[1/1000][2100/3166] Loss_D: 5.8933 Loss_G: 0.1496 D(x): 0.0704 D(G(z)): 0.7380 / 0.8746\n",
      "[1/1000][2200/3166] Loss_D: 7.1908 Loss_G: 0.1372 D(x): 0.0992 D(G(z)): 0.9280 / 0.8967\n",
      "[1/1000][2300/3166] Loss_D: 8.5645 Loss_G: 0.4338 D(x): 0.0705 D(G(z)): 0.8755 / 0.7126\n",
      "[1/1000][2400/3166] Loss_D: 7.0863 Loss_G: 0.1536 D(x): 0.0979 D(G(z)): 0.8940 / 0.8786\n",
      "[1/1000][2500/3166] Loss_D: 6.1001 Loss_G: 0.3725 D(x): 0.0986 D(G(z)): 0.8438 / 0.7513\n",
      "[1/1000][2600/3166] Loss_D: 7.5359 Loss_G: 0.6766 D(x): 0.0245 D(G(z)): 0.6394 / 0.6121\n",
      "[1/1000][2700/3166] Loss_D: 6.6799 Loss_G: 0.1767 D(x): 0.1648 D(G(z)): 0.9274 / 0.8693\n",
      "[1/1000][2800/3166] Loss_D: 7.1453 Loss_G: 0.1810 D(x): 0.1144 D(G(z)): 0.8857 / 0.8618\n",
      "[1/1000][2900/3166] Loss_D: 7.4202 Loss_G: 0.2693 D(x): 0.1113 D(G(z)): 0.8779 / 0.8011\n",
      "[1/1000][3000/3166] Loss_D: 6.6175 Loss_G: 1.1529 D(x): 0.4786 D(G(z)): 0.9738 / 0.4337\n",
      "[1/1000][3100/3166] Loss_D: 9.0487 Loss_G: 0.4683 D(x): 0.0062 D(G(z)): 0.6161 / 0.7117\n",
      "[2/1000][0/3166] Loss_D: 6.8512 Loss_G: 0.2155 D(x): 0.0788 D(G(z)): 0.8517 / 0.8189\n",
      "[2/1000][100/3166] Loss_D: 7.0593 Loss_G: 0.3085 D(x): 0.0706 D(G(z)): 0.8486 / 0.7906\n",
      "[2/1000][200/3166] Loss_D: 4.4155 Loss_G: 0.1656 D(x): 0.0981 D(G(z)): 0.6155 / 0.8577\n",
      "[2/1000][300/3166] Loss_D: 6.1004 Loss_G: 0.2948 D(x): 0.1646 D(G(z)): 0.9054 / 0.7676\n",
      "[2/1000][400/3166] Loss_D: 5.7725 Loss_G: 0.4232 D(x): 0.0989 D(G(z)): 0.8196 / 0.7164\n",
      "[2/1000][500/3166] Loss_D: 5.7912 Loss_G: 0.2020 D(x): 0.1877 D(G(z)): 0.8760 / 0.8339\n",
      "[2/1000][600/3166] Loss_D: 4.1601 Loss_G: 0.4362 D(x): 0.2942 D(G(z)): 0.7430 / 0.7312\n",
      "[2/1000][700/3166] Loss_D: 5.7785 Loss_G: 0.4025 D(x): 0.3620 D(G(z)): 0.9497 / 0.7271\n",
      "[2/1000][800/3166] Loss_D: 6.6579 Loss_G: 0.1173 D(x): 0.0603 D(G(z)): 0.8489 / 0.8964\n",
      "[2/1000][900/3166] Loss_D: 8.2732 Loss_G: 0.4882 D(x): 0.0313 D(G(z)): 0.7797 / 0.7145\n",
      "[2/1000][1000/3166] Loss_D: 5.1937 Loss_G: 0.0801 D(x): 0.1137 D(G(z)): 0.7842 / 0.9261\n",
      "[2/1000][1100/3166] Loss_D: 6.5766 Loss_G: 0.2930 D(x): 0.0718 D(G(z)): 0.8115 / 0.7924\n",
      "[2/1000][1200/3166] Loss_D: 4.8553 Loss_G: 0.2960 D(x): 0.1837 D(G(z)): 0.7547 / 0.7793\n",
      "[2/1000][1300/3166] Loss_D: 6.1495 Loss_G: 0.4205 D(x): 0.0947 D(G(z)): 0.8396 / 0.6877\n",
      "[2/1000][1400/3166] Loss_D: 6.3799 Loss_G: 0.3425 D(x): 0.0352 D(G(z)): 0.7887 / 0.7408\n",
      "[2/1000][1500/3166] Loss_D: 4.4522 Loss_G: 0.3422 D(x): 0.2530 D(G(z)): 0.7517 / 0.7425\n",
      "[2/1000][1600/3166] Loss_D: 5.0037 Loss_G: 0.5049 D(x): 0.2157 D(G(z)): 0.8036 / 0.6616\n",
      "[2/1000][1700/3166] Loss_D: 6.1571 Loss_G: 0.2999 D(x): 0.5255 D(G(z)): 0.9866 / 0.7658\n",
      "[2/1000][1800/3166] Loss_D: 5.0961 Loss_G: 0.3704 D(x): 0.2959 D(G(z)): 0.8770 / 0.7367\n",
      "[2/1000][1900/3166] Loss_D: 7.1092 Loss_G: 0.3417 D(x): 0.0292 D(G(z)): 0.7511 / 0.7738\n",
      "[2/1000][2000/3166] Loss_D: 5.1056 Loss_G: 0.3488 D(x): 0.2010 D(G(z)): 0.8574 / 0.7623\n",
      "[2/1000][2100/3166] Loss_D: 5.6686 Loss_G: 0.4426 D(x): 0.0762 D(G(z)): 0.6349 / 0.6960\n",
      "[2/1000][2200/3166] Loss_D: 6.1995 Loss_G: 0.2700 D(x): 0.0583 D(G(z)): 0.7970 / 0.7877\n",
      "[2/1000][2300/3166] Loss_D: 6.4924 Loss_G: 0.6687 D(x): 0.1102 D(G(z)): 0.8809 / 0.6144\n",
      "[2/1000][2400/3166] Loss_D: 6.2277 Loss_G: 0.9352 D(x): 0.1064 D(G(z)): 0.8698 / 0.4693\n",
      "[2/1000][2500/3166] Loss_D: 5.9765 Loss_G: 0.2917 D(x): 0.0941 D(G(z)): 0.8544 / 0.7991\n",
      "[2/1000][2600/3166] Loss_D: 5.9576 Loss_G: 0.0766 D(x): 0.1477 D(G(z)): 0.8735 / 0.9317\n",
      "[2/1000][2700/3166] Loss_D: 5.2391 Loss_G: 0.0049 D(x): 0.0893 D(G(z)): 0.1874 / 0.9951\n",
      "[2/1000][2800/3166] Loss_D: 4.8810 Loss_G: 0.2472 D(x): 0.2456 D(G(z)): 0.8544 / 0.8134\n",
      "[2/1000][2900/3166] Loss_D: 3.8873 Loss_G: 1.2645 D(x): 0.3385 D(G(z)): 0.7285 / 0.3703\n",
      "[2/1000][3000/3166] Loss_D: 5.2506 Loss_G: 0.1846 D(x): 0.1797 D(G(z)): 0.8482 / 0.8445\n",
      "[2/1000][3100/3166] Loss_D: 5.0617 Loss_G: 0.2529 D(x): 0.1557 D(G(z)): 0.8463 / 0.8094\n",
      "[3/1000][0/3166] Loss_D: 5.5141 Loss_G: 0.1757 D(x): 0.1332 D(G(z)): 0.8716 / 0.8598\n",
      "[3/1000][100/3166] Loss_D: 4.5059 Loss_G: 0.3290 D(x): 0.2864 D(G(z)): 0.8529 / 0.7394\n",
      "[3/1000][200/3166] Loss_D: 4.2131 Loss_G: 0.9520 D(x): 0.2303 D(G(z)): 0.5811 / 0.5472\n",
      "[3/1000][300/3166] Loss_D: 4.8038 Loss_G: 0.2902 D(x): 0.2112 D(G(z)): 0.8568 / 0.7727\n",
      "[3/1000][400/3166] Loss_D: 5.2354 Loss_G: 0.2367 D(x): 0.1187 D(G(z)): 0.8004 / 0.8145\n",
      "[3/1000][500/3166] Loss_D: 4.6949 Loss_G: 0.3857 D(x): 0.1772 D(G(z)): 0.8054 / 0.7309\n",
      "[3/1000][600/3166] Loss_D: 6.2905 Loss_G: 0.3409 D(x): 0.0705 D(G(z)): 0.8226 / 0.7464\n",
      "[3/1000][700/3166] Loss_D: 3.9335 Loss_G: 0.3458 D(x): 0.4031 D(G(z)): 0.8840 / 0.7304\n",
      "[3/1000][800/3166] Loss_D: 6.0082 Loss_G: 0.2456 D(x): 0.2794 D(G(z)): 0.9500 / 0.8121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-249:\n",
      "Process Process-250:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b63968082cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0merrD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0merrD_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mD_G_z1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0merrD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrD_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merrD_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/kamil/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if use_cuda:\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        inputv = Variable(input)\n",
    "        labelv = Variable(label)\n",
    "\n",
    "        output = netD(inputv)\n",
    "        errD_real = criterion(output, labelv)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        noise.resize_(batch_size, latent_dim, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, 1000, i, len(dataloader),\n",
    "                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            #plt.imshow(image)\n",
    "            #vutils.save_image(real_cpu,\n",
    "             #       '%s/real_samples.png' % path_output,\n",
    "              #      normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "\n",
    "            #print(np.swapaxes(im,0,2).shape)\n",
    "            #show(make_grid(image, padding=100, normalize=True))\n",
    "            #vutils.save_image(fake.data,\n",
    "            #        '%s/fake_samples_epoch_%03d.png' % (\"output\", epoch),\n",
    "            #        normalize=True)\n",
    "\n",
    "    #show result\n",
    "    fake = netG(fixed_noise)\n",
    "    vutils.save_image(fake.data,\n",
    "            '%s/fake_samples_epoch_%03d.png' % (\"output\", epoch),\n",
    "            normalize=True)\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (path_output, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (path_output, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
